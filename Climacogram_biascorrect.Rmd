```{r message=FALSE, warning=FALSE}
library(dplyr)
library(readxl)
library(ggplot2)
library(lubridate)
library(DEoptim)
```



```{r}
gdrive_path <- paste(LETTERS[file.exists(paste0(LETTERS, ":/My Drive"))],":/My Drive",sep = "")
project_path <- file.path(gdrive_path,"Academic_git/DownScaling")
generalData_path <- file.path(gdrive_path,"General_Data")
```





```{r}
library(dplyr)
library(lubridate)

# --- 1 & 2. READ DATA AND DATES (Same as before) ---
file_path <- file.path(generalData_path, "EL08/makrinitsa_YPEN_.txt")
df <- read.table(file_path, skip = 16, col.names = "precip")

df$precip[df$precip == -999] <- NA

header_lines <- readLines(file_path, n = 16)
start_date_str <- sub("Start datetime: ", "", grep("^Start datetime:", header_lines, value = TRUE))
end_date_str <- sub("End datetime: ", "", grep("^End datetime:", header_lines, value = TRUE))

start_ts <- ymd(start_date_str)
end_ts <- ymd(end_date_str)
df$date <- seq(start_ts, end_ts, by = "day")

# --- 3. CONVERT TO AVERAGED PROCESS ---
intervals_per_day <- 96
df$precip_rate <- df$precip / intervals_per_day
df$day_index <- as.numeric(df$date - min(df$date)) 

# --- 4. AGGREGATE, INSPECT, AND CALCULATE VARIANCE ---
aggregation_days <- 1:10
empirical_k <- aggregation_days * intervals_per_day 
empirical_var <- numeric(length(aggregation_days))

# Create an empty list to store our inspection dataframes
inspection_list <- list()

for (i in seq_along(aggregation_days)) {
  agg_length <- aggregation_days[i]
  
  # 1. Create the detailed inspection dataframe
  df_detailed <- df %>%
    mutate(bin_id = day_index %/% agg_length) %>%
    group_by(bin_id) %>%
    mutate(
      days_in_bin = n(),
      bin_mean_precip = mean(precip_rate, na.rm = FALSE),
      is_valid_bin = (!is.na(bin_mean_precip) & days_in_bin == agg_length)
    ) %>%
    ungroup()
  
  # 2. Save the dataframe to our list for later inspection
  list_name <- paste0("scale_", agg_length, "d")
  inspection_list[[list_name]] <- df_detailed
  
  # 3. Extract the clean vector of valid bin means
  valid_bins <- df_detailed %>%
    filter(is_valid_bin) %>%
    distinct(bin_id, bin_mean_precip)
  
  # --- MANUAL VARIANCE CALCULATION ---
  X <-  valid_bins$bin_mean_precip
 
  
  empirical_var[i] <- var(X)
  
}


# --- 5. RESULTS SUMMARY ---
results <- data.frame(
  scale_days = aggregation_days,
  scale_k = empirical_k,
  variance = empirical_var
)


Insp<-inspection_list$scale_1d %>%
  group_by(bin_id) %>%
  summarise(common_value = unique(bin_mean_precip))

var(Insp$common_value,na.rm = TRUE)


plot(results$variance)

```



```{r}

T_intervals <- nrow(df) * intervals_per_day 


gamma_theory <- function(k, lambda, alpha, M, H) {
  lambda * (1 + (k / alpha)^(2 * M))^((H - 1) / M)
}

expected_S2 <- function(k, lambda, alpha, M, H, T_total) {
  n_k <- T_total / k
  correction_factor <- ifelse(n_k > 1, n_k / (n_k - 1), 1)
  correction_factor * (gamma_theory(k, lambda, alpha, M, H) - 
                         gamma_theory(T_total, lambda, alpha, M, H))
}

fit_data <- results[!is.na(results$variance) & results$variance > 0, ]

fit_data<-fit_data[,2:3]

fit_bias_corrected <- nls(
  variance ~ expected_S2(scale_k, lambda, alpha, M, H, T_intervals), 
  data = fit_data,
  start = list(lambda = max(fit_data$variance), alpha = 96, M = 0.5, H = 0.7),
  lower = list(lambda = 0, alpha = 0.01, M = 0.01, H = 0.01),
  upper = list(lambda = Inf, alpha = Inf, M = 0.99, H = 0.99),
  algorithm = "port", 
  control = nls.control(maxiter = 1000, warnOnly = TRUE)
)


params <- coef(fit_bias_corrected)

true_var_15min <- gamma_theory(k = 1, 
                               lambda = params["lambda"], 
                               alpha = params["alpha"], 
                               M = params["M"], 
                               H = params["H"])

cat("Estimated Parameters:\n")
print(params)
cat("\nBias-Corrected 15-min Variance Extrapolation (k=1):", true_var_15min, "\n")




```















