```{r message=FALSE, warning=FALSE}
library(dplyr)
library(readxl)
library(ggplot2)
library(lubridate)
```



```{r}
gdrive_path <- paste(LETTERS[file.exists(paste0(LETTERS, ":/My Drive"))],":/My Drive",sep = "")
project_path <- file.path(gdrive_path,"Academic_git/DownScaling")
generalData_path <- file.path(gdrive_path,"General_Data")
```




```{r}
# 2. Read the data 
df <- read_excel(file.path(generalData_path,"EL08_precipitation.xlsx"),col_names = FALSE)

colnames(df)[2] <- "datetime"
colnames(df)[3:ncol(df)]<-seq(3,ncol(df),1)

df$datetime<-format(as.Date(df$datetime), "%Y%m%d")


for (station_column in 3:ncol(df)) {
 


station_column<-3

#3 is the station
values <-df[[station_column]][8:nrow(df)]

values[is.na(values) | values == ""] <- c(-999)
  
starting_index<-7+which(values!="-999")[1]
end_index<-7+tail(which(values!="-999"),1)
 
 
start_dt <- df$datetime[starting_index]
end_dt <- df$datetime[end_index]
 
num_records <- end_index-starting_index
percent_missing <- round((sum(values[starting_index:end_index] == -999) / num_records) * 100, 2)
  

  header <- c(
    paste0("Station ID: ", paste0("EL04_S",station_column-2)),
    paste0("Country: Greece"),  
    paste0("Original Station Name: ",  df[[station_column]][1]),
    paste0("Source: ", "WD"),   
    paste0("Owner: ",df[[station_column]][5]),
    paste0("X: ", df[[station_column]][2]), 
    paste0("Y: ", df[[station_column]][3]), 
    paste0("Z: ", df[[station_column]][4]), 
    paste0("Start datetime: ", start_dt),
    paste0("End datetime: ", end_dt),
    paste0("Number of records: ", num_records),
    paste0("Percent missing data: ", format(percent_missing, nsmall=2)),
    "Original Timestep: 24hr",
    "Original Units: mm",
    "No data value: -999",""
  )
  
  # Combine header and the data values
  output_lines <- c(header, values[starting_index:end_index])
 
  # Write to the text file in the current WD
  output_filename <- paste0(df[[station_column]][1],"_",df[[station_column]][5], "_.txt")
  writeLines(output_lines, output_filename)
  
}



```


```{r}
file_path <- file.path(generalData_path,"EL08/agchialos_EMY_.txt")

header_lines <- readLines(file_path, n = 17)
start_line <- grep("^Start datetime:", header_lines, value = TRUE)
start_date_str <- sub("Start datetime: ", "", start_line)
start_date <- as.Date(start_date_str, format = "%Y%m%d")

df <- read.table(file_path, skip = 17,col.names = c("Precipitation_mm"))
df$Precipitation_mm[df$Precipitation_mm == -999] <- NA
df$Date <- seq(start_date, by = "day", length.out = nrow(df))


ggplot(df, aes(x = Date, y = Precipitation_mm)) +
  geom_line(color = "blue", linewidth = 0.5) +
  labs(
    title = paste("Daily Precipitation (Starting:", start_date, ")"),
    x = "Date",
    y = "Precipitation (mm)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12)
  )

```



```{r}

# --- 1. SET YOUR AGGREGATION LENGTH ---
# You can change this number to whatever you want (e.g., 4, 7, 10)
agg_length <- 4 

# --- 2. READ THE DATA ---
# Skip the 15 lines of metadata at the top of the text file
file_path <- file.path(generalData_path,"EL08/agchialos_EMY_.txt")
df <- read.table(file_path, skip = 16, col.names = "precip")

# --- 3. CLEAN MISSING VALUES ---
# The metadata says "-999" is the 'no data' value. We convert these to standard R NAs.
df$precip[df$precip == -999] <- NA

# --- 4. ADD DATES ---
# Create a sequence of dates based on the start and end dates in the file header

header_lines <- readLines(file_path, n = 16)

start_line <- grep("^Start datetime:", header_lines, value = TRUE)
start_date_str <- sub("Start datetime: ", "", start_line)


end_line <- grep("^End datetime:", header_lines, value = TRUE)
end_date_str <- sub("End datetime: ", "", end_line)# sensitive even in spacebar


start_ts <- ymd(start_date_str)
end_ts <- ymd(end_date_str)

df$date <- seq(start_ts, end_ts, by = "day")


# --- 5. ASSIGN BINS BASED ON THE CALENDAR YEAR ---
df <- df %>%
  mutate(
    # Shift every date forward by 3 months. 
    # October 1st becomes January 1st!
    Hdate = date %m+% months(3),
    
    # Now just extract the year from the shifted date
    Hyear = year(Hdate),
    
    # Because Oct 1st is now Jan 1st, yday() perfectly starts at 1
    bin_in_Hyear = (yday(Hdate) - 1) %/% agg_length
  )


# --- 6. AGGREGATE THE BINS ---
df_aggregated <- df %>%
  # Group by both the Year and the specific Bin ID we just created
  group_by(Hyear, bin_in_Hyear) %>%
  summarise(
    bin_start_date = min(date),
    bin_end_date = max(date),
    # Count how many days actually fell into this bin
    days_in_bin = n(),
    
    # Calculate the sum. 
    # na.rm = FALSE ensures that if there is even one NA, the whole sum becomes NA.
    total_precip = sum(precip, na.rm = FALSE),
    
    .groups = "drop" # Clean up the grouping
  )


# --- 7. APPLY REJECTION RULES ---
# We want to reject the bin if:
# a) It contains an NA value (total_precip became NA in the step above)
# b) It doesn't have the full number of days (e.g., leftover days at the end of the year)
df_final <- df_aggregated %>%
  filter(!is.na(total_precip) & days_in_bin == agg_length) %>%
  # Keep only the dates and the final summed value
  select(bin_start_date, bin_end_date, total_precip)

# View the final result
head(df_final)
```













```{r}

library(dplyr)
library(lubridate)

# --- 1 & 2. READ THE DATA AND DATES ---
file_path <- file.path(generalData_path,"EL08/agchialos_EMY_.txt")
df <- read.table(file_path, skip = 16, col.names = "precip")

# Clean NAs
df$precip[df$precip == -999] <- NA

# Extract dates dynamically from header
header_lines <- readLines(file_path, n = 16)
start_line <- grep("^Start datetime:", header_lines, value = TRUE)
start_date_str <- sub("Start datetime: ", "", start_line)
end_line <- grep("^End datetime:", header_lines, value = TRUE)
end_date_str <- sub("End datetime: ", "", end_line)

start_ts <- ymd(start_date_str)
end_ts <- ymd(end_date_str)
df$date <- seq(start_ts, end_ts, by = "day")

# --- 3. CONVERT TO AVERAGED PROCESS (CRITICAL FOR CLIMACOGRAM) ---
# Since your goal is 15-minute statistics, k=1 is 15 minutes. 
# There are 96 15-minute intervals in a day (24 hours * 4). 
# We divide the daily accumulated volume by 96 to get the 
# "averaged process" intensity.
intervals_per_day <- 96
df$precip_rate <- df$precip / intervals_per_day

# --- 4. CALCULATE VARIANCES ACROSS MULTIPLE SCALES ---
# Define the blocks of days we want to aggregate over
aggregation_days <- c(1:5)

# Convert days into the fundamental scale k (number of 15-min intervals)
empirical_k <- aggregation_days * intervals_per_day 
empirical_var <- numeric(length(aggregation_days))

# Loop your exact binning logic over the different aggregation lengths
for (i in seq_along(aggregation_days)) {
  
  agg_length <- aggregation_days[i]
  
  # Standard calendar year binning and mean calculation
  df_aggregated <- df %>%
    mutate(
      # Extract the standard calendar year
      cal_year = year(date),
      
      # Group by the standard day of the year (starts perfectly at Jan 1st)
      bin_in_year = (yday(date) - 1) %/% agg_length
    ) %>%
    # Group by the standard year and bin
    group_by(cal_year, bin_in_year) %>%
    summarise(
      days_in_bin = n(),
      mean_precip = mean(precip_rate, na.rm = FALSE),
      .groups = "drop"
    ) %>%
    # Your exact rejection rules
    filter(!is.na(mean_precip) & days_in_bin == agg_length)
  
  # Extract the variance of the block means for this specific scale
  empirical_var[i] <- var(df_aggregated$mean_precip)
}

# --- 5. DEFINE CLIMACOGRAM & OPTIMIZATION ---
# [cite_start]Cauchy-type climacogram (Equation 6 from Kossieris et al., 2021) [cite: 198]
climacogram <- function(k, gamma_1, alpha, H, M) {
  gamma_1 * ( (alpha^(2*M) + k^(2*M)) / (alpha^(2*M) + 1) ) ^ ((H - 1) / M)
}



# [cite_start]Objective function minimizing squared relative difference (Equation 2) [cite: 143]
objective_function <- function(params) {
  gamma_1 <- params[1]
  alpha   <- params[2]
  H       <- params[3]
  M       <- params[4]
  
  modeled_var <- climacogram(empirical_k, gamma_1, alpha, H, M)
  
  # Minimize sum of squared relative errors
  error <- sum( ((empirical_var - modeled_var) / empirical_var)^2 )
  return(error)
}

# --- 6. RUN OPTIMIZATION TO DOWNSCALE ---
# [cite_start]Initial guesses and constraints (H and M must be in (0,1]) [cite: 203]
initial_params <- c(gamma_1 = empirical_var[1] , alpha = 10, H = 0.5, M = 0.5)
lower_bounds <- c(gamma_1 = 1e-6, alpha = 1e-6, H = 1e-3, M = 1e-6)
upper_bounds <- c(gamma_1 = Inf,  alpha = 1000, H = 1.0,  M = 1)

# Run L-BFGS-B bounded optimization
fit <- optim(
  par = initial_params,
  fn = objective_function,
  method = "L-BFGS-B",
  lower = lower_bounds,
  upper = upper_bounds,control = list(maxit = 50000, factr = 1e7)
)

opt_params <- fit$par
gamma_1_15min <- opt_params["gamma_1"]
alpha_opt     <- opt_params["alpha"]
H_opt         <- opt_params["H"]
M_opt         <- opt_params["M"]

cat("\n--- DOWNSCALED VARIANCE RESULT ---\n")
cat("Estimated 15-minute Variance (gamma_1):", round(gamma_1_15min, 6), "\n")
cat("Fitted Parameters -> alpha:", round(alpha_opt, 2), 
    "| H:", round(H_opt, 3), 
    "| M:", round(M_opt, 3), "\n\n")

# --- 7. DERIVE 15-MINUTE AUTOCOVARIANCE ---
# [cite_start]Based on Equation (4) from the paper [cite: 186]
Gamma_agg <- function(k) {
  if (k == 0) return(0)
  k^2 * climacogram(k, gamma_1_15min, alpha_opt, H_opt, M_opt) # Equation 3 [cite: 182]
}

get_autocovariance <- function(tau, k_scale = 1) {
  term1 <- Gamma_agg(abs(tau + 1) * k_scale)
  term2 <- Gamma_agg(abs(tau - 1) * k_scale)
  term3 <- Gamma_agg(abs(tau) * k_scale)
  
  (1 / k_scale^2) * ( (term1 + term2) / 2 - term3 )
}

cat("--- 15-MINUTE AUTOCOVARIANCE (Lag 0 to 3) ---\n")
for (tau in 0:3) {
  c_tau <- get_autocovariance(tau = tau, k_scale = 1)
  cat(sprintf("Lag %d ( %2d mins ): %f\n", tau, tau*15, c_tau))
}





```








```{r}
# --- 8. PREPARE THE DATA FOR PLOTTING ---

# Empirical data points (from your aggregated daily data)
df_empirical <- data.frame(
  k = empirical_k,
  variance = empirical_var,
  type = "Observed (Coarse Scales)"
)

# Generate a continuous sequence of k values to draw the smooth fitted model line
# We sequence from k=1 (15-min) to the maximum coarse scale we used
k_seq <- exp(seq(log(1), log(max(empirical_k)), length.out = 200))
modeled_var_seq <- climacogram(k_seq, gamma_1_15min, alpha_opt, H_opt, M_opt)

df_model <- data.frame(
  k = k_seq,
  variance = modeled_var_seq,
  type = "Fitted Model"
)

# The single downscaled 15-minute point
df_downscaled <- data.frame(
  k = 1,
  variance = gamma_1_15min,
  type = "Downscaled (15-min)"
)

# --- 9. CREATE THE LOG-LOG PLOT ---

ggplot() +
  # Draw the continuous fitted model line first (like the black line in the paper)
  geom_line(data = df_model, aes(x = k, y = variance, color = "Fitted Model"), size = 0.8) +
  
  # Add the observed coarse scale points (like the orange points in the paper)
  geom_point(data = df_empirical, aes(x = k, y = variance, color = "Observed (Coarse Scales)"), size = 3) +
  
  # Add the extrapolated downscaled point at k=1
  geom_point(data = df_downscaled, aes(x = k, y = variance, color = "Downscaled (15-min)"), size = 4, shape = 18) +
  
  # Set both axes to logarithmic scales as done in the paper
  # Notice the breaks are now multiples of 96 (since 1 day = 96 intervals of 15 mins)
  scale_x_log10(breaks = c(1, 96, 96*10, 96*30),
                labels = c("15-min (k=1)", "1 Day (k=96)", "10 Days", "30 Days")) +
  
  # Customize colors to match the paper's aesthetic
  scale_color_manual(values = c("Observed (Coarse Scales)" = "orange", 
                                "Fitted Model" = "black", 
                                "Downscaled (15-min)" = "dodgerblue")) +
  
  # Add labels and formatting
  labs(
    title = "Variance Downscaling via Cauchy-type Climacogram",
    subtitle = sprintf("Extrapolated 15-min variance: %.4f | H: %.2f | M: %.2f", 
                       gamma_1_15min, H_opt, M_opt),
    x = "Scale k (number of 15-min intervals)",
    y = "Variance ",
    color = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  )



```



